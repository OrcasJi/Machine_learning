{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NRMjCyyZdMy"
      },
      "source": [
        "## 0. Introduction\n",
        "\n",
        "The aim of this lab is to get familiar with **regression problems**, the concepts of **under/over-fitting**, and **regularization**.\n",
        "\n",
        "\n",
        "1.   This lab is the first course-work activity **Assignment 1 Part 1: Regression**\n",
        "2. Assignment 1 is split into 3 parts: Part 1 - Regression (Lab 3), and Part 2 - Classification (Lab 4) and Part 3 - Neural Networks (Lab 6). The three parts need to be submitted together by  **Monday, 11th November 2024, 10:00am**\n",
        "3. All questions need to be completed and are assessed.\n",
        "4. For each lab, a report answering the <font color = 'red'>**questions in red**</font> should be included along with the completed Notebook (.ipynb)\n",
        "5. The report should be a separate file in **pdf format** (so **NOT** *doc, docx, notebook* etc.).\n",
        "6. Make sure that **any figures or code** you comment on, are **included in the report**.\n",
        "7. There are three parts of Assignment 1, therefore you should submit a zip file that contains 1 x Notebook and 1 x PDF for each part. In total: 3 x Notebooks and 3 x PDFs in the submission zip. Make sure the file is well identified with your name, student number, assignment number (for instance, Assignment 1), module code.\n",
        "7. No other means of submission other than the appropriate QM+ link is acceptable at any time (so NO email attachments, etc.)\n",
        "8. **PLAGIARISM** <ins>is an irreversible non-negotiable failure in the course</ins> (if in doubt of what constitutes plagiarism, ask!).\n",
        "\n",
        "\n",
        "For this lab we will use the [diabetes](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html) dataset. Our task is to predict disease progression of diabetes, given some variables such as age, sex, body mass index, blood pressure and blood serum measurements. For more details, follow the link above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkLnfbLLwxxS"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '.venv (Python 3.13.0)' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'd:/Python/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from sklearn import model_selection\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from IPython import display\n",
        "\n",
        "import typing\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJJVj3d3b43I"
      },
      "outputs": [],
      "source": [
        "diabetes_db = pd.read_csv('https://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt', sep='\\t', header=0)\n",
        "sn.pairplot(diabetes_db)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imFNxOVrf36P"
      },
      "outputs": [],
      "source": [
        "diabetes_db.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGjbgZxNdrLH"
      },
      "source": [
        "We first split the data into test and training sets. For consistency and to allow for meaningful comparison the same splits are maintained in the remainder of the lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lad3ihXjd-Ev"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
        "    diabetes_db.loc[:, diabetes_db.columns != 'Y'],\n",
        "    diabetes_db['Y'],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        "    )\n",
        "x_train = torch.from_numpy(X_train.values).float()\n",
        "x_test = torch.from_numpy(X_test.values).float()\n",
        "\n",
        "y_train = torch.from_numpy(y_train.values).float()\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "\n",
        "y_test = torch.from_numpy(y_test.values).float()\n",
        "y_test = y_test.reshape(-1, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLLITe4POndx"
      },
      "source": [
        "We can see that all the independent variables are on different scales. This can affect gradient descent, we therefore need to normalize all features to zero mean, and unit standard deviation. The normalized value $z_i$ of $x_i$ is obtained through $z_i = \\frac{x_i - μ}{σ}$ where $μ$ is the mean and $σ$ is the standard deviation of $X$ and $x_i, μ, σ ∈ \\mathbb{R}^D$.\n",
        "\n",
        "**Q1.** Complete the method and normalize `x_train, x_test` [2 marks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3E-8C1USNFP"
      },
      "outputs": [],
      "source": [
        "def norm_set(x: torch.Tensor, mu: torch.Tensor, sigma: torch.Tensor) -> torch.tensor:\n",
        "  ### your code here\n",
        "  return\n",
        "\n",
        "###your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_qQkwRGFBF8"
      },
      "source": [
        "## 1.1 Linear Regression\n",
        "\n",
        "We will building the linear regression model in pytorch using a custom layer.\n",
        "\n",
        "Refering back to the lecture notes, we define $ y = f(x) = w^T x$, so we need to learn weight vector $w$.\n",
        "\n",
        "**Q2.** Fill in the forward method of the LinearRegression class. [2 marks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxp1LoliF3Qc"
      },
      "outputs": [],
      "source": [
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self, num_features):\n",
        "    super().__init__()\n",
        "    self.weight = nn.Parameter(torch.ones(1, num_features), requires_grad=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y = 0\n",
        "    ### your code here\n",
        "    return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr7MDAIWboLi"
      },
      "source": [
        "As we need to account for the bias, we add a column of ones to the `x_data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCn-E-CZbxOL"
      },
      "outputs": [],
      "source": [
        "# add a feature for bias\n",
        "x_train = torch.cat([x_train, torch.ones(x_train.shape[0], 1)], dim=1)\n",
        "x_test = torch.cat([x_test, torch.ones(x_test.shape[0], 1)], dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tfeh3o4Gk_xW"
      },
      "outputs": [],
      "source": [
        "## test the custom layer\n",
        "model = LinearRegression(x_train.shape[1])\n",
        "prediction = model(x_train)\n",
        "prediction.shape # the output should be Nx1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-kUQVD9ne_i"
      },
      "source": [
        "The next step is to calculate the cost. For this we will use the mean squared error $E(w) = \\frac{1}{N} Σ_{i=0}^N (f(x_i) - y_i)^2$\n",
        "\n",
        "**Q3.** Fill in the method to calculate the squared error of for any set of labels $y$ and predictions [2 marks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25SVCd01l29-"
      },
      "outputs": [],
      "source": [
        "def mean_squared_error(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
        "  ### your code here\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTuAgK6FpLEh"
      },
      "outputs": [],
      "source": [
        "cost = mean_squared_error(y_train, prediction)\n",
        "print(cost)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVV7MCfxqtyA"
      },
      "source": [
        "We see that using a random set of initial parameters for bias and weight, yields a relatively high error. As such, we will update the values for $w$ using gradient descent. We will implement a custom method for gradient descent.\n",
        "\n",
        "**Q4.** In the method below, add your code to update bias and weight using learning rate $α$. [2 marks]\n",
        "\n",
        "First you need to calculate the partial derivative of the loss function with respect to the weights.\n",
        "\n",
        "We then update the weights vector using the following equation:\n",
        "\n",
        "$ weight = weight - α * ∂_{weight}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJXOTQInsIFz"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_step(model: nn.Module, X: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor, lr: float) -> None:\n",
        "  weight = model.weight\n",
        "  N = X.shape[0]\n",
        "  ### your code here\n",
        "  # calculate the partial derivative of the loss function with respect to w\n",
        "  # calculate the new values the weights\n",
        "  model.weight = nn.Parameter(weight, requires_grad=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lQDX_zeG6W1"
      },
      "outputs": [],
      "source": [
        "cost_lst = list()\n",
        "model = LinearRegression(x_train.shape[1])\n",
        "alpha = .1\n",
        "for it in range(100):\n",
        "  prediction = model(x_train)\n",
        "  cost = mean_squared_error(y_train, prediction)\n",
        "  cost_lst.append(cost)\n",
        "  gradient_descent_step(model, x_train, y_train, prediction, alpha)\n",
        "fig, axs = plt.subplots(2)\n",
        "axs[0].plot(list(range(it+1)), cost_lst)\n",
        "axs[1].scatter(prediction, y_train)\n",
        "plt.show()\n",
        "print(model.weight)\n",
        "print('Minimum cost: {}'.format(min(cost_lst)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwypj_XWLKKq"
      },
      "source": [
        "<font color=\"maroon\">**Q5.** What conclusion if any can be drawn from the weight values? How does sex and BMI affect diabetes disease progression?\n",
        "\n",
        "What are the estimated disease progression values for the below examples? [2 marks] </font>\n",
        "\n",
        "| AGE | SEX | BMI | BP  | S1  | S2    | S3 | S4  | S5     | S6  |\n",
        "|-----|-----|-----|-----|-----|-------|----|-----|--------|-----|\n",
        "| 25  | F   | 18  | 79  | 130 | 64.8  | 61 | 2   | 4.1897 | 68  |\n",
        "| 50  | M   | 28  | 103 | 229 | 162.2 | 60 | 4.5 | 6.107  | 124 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StYxCUdPWSEp"
      },
      "outputs": [],
      "source": [
        "### your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IlXN47P-cCY"
      },
      "source": [
        "Now estimate the error on the test set. Is the error on the test set comparable to that of the train set? What can be said about the fit of the model? When does a model over/under fits?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yuYH06A-xoj"
      },
      "outputs": [],
      "source": [
        "### your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnz2O9Pagpf7"
      },
      "source": [
        "<font color=\"maroon\">**Q6.** Try the code with several learning rates that differ by orders of magnitude, and record the training and test set errors. Describe the theory of how changing the learning rate affects learning. What do you observe in the training error? How about the error on the test set? [3 marks]</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp9nNM0R-7CH"
      },
      "outputs": [],
      "source": [
        "### your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI9cKY_TWiUM"
      },
      "source": [
        "# 1.2 Regularized Linear Regression\n",
        "In this exercise, we will be trying to create a model that fits data that is clearly not linear. We will be attempting to fit the data points seen in the graph below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hupAUhrofKfy"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([-0.99768, -0.69574, -0.40373, -0.10236, 0.22024, 0.47742, 0.82229])\n",
        "y = torch.tensor([2.0885, 1.1646, 0.3287, 0.46013, 0.44808, 0.10013, -0.32952]).reshape(-1, 1)\n",
        "plt.scatter(x, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6epuBvnf7P9"
      },
      "source": [
        "In order to fit this data we will create a new hypothesis function, which uses a fifth-order polynomial:\n",
        "\n",
        "$\n",
        " h_{\\theta}(x) = \\theta_{0} x_{0} + \\theta_{1} x_{1} + \\theta_{2} x_{1} ^2 + \\theta_{3} x_{1} ^3 + \\theta_{4} x_{1} ^4 + \\theta_{5} x_{1} ^5\n",
        "$\n",
        "\n",
        "As we are fitting a small number of points with a high order model, there is a danger of overfitting.  \\\\\n",
        "\n",
        "To attempt to avoid this we will use regularization. Our cost function becomes:\n",
        "\n",
        "$\n",
        " J(\\theta) = \\frac{1}{2m}  \\left[ \\sum_{i=1}^m ( h_{\\theta}(x^{(i)}) - y^{(i)})^2 + \\lambda    \\sum_{j=1}^n \\theta_{j}^2 \\right]\n",
        "$\n",
        "\n",
        "Adjust variable `x` to include the different powers of $x_{1}$ as described by the hypothesis function. e.g $x_{1}, x_{1} ^2, ..., x_{1}^5$. Hint: Remember to include $x_0$ which is our bias term."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nOF1xJKY_Jx"
      },
      "outputs": [],
      "source": [
        "### your code here. please assign it to the python object 'x3'. e.g x3 = ....."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg9OdKcyhEN0"
      },
      "source": [
        "**Q7.** Update the cost and gradient descent methods to use the regularised cost, as shown above. [4 marks]\n",
        "\n",
        "Note that the punishment for having more terms is not applied to the bias. This means that we use a different update technique for the partial derivative of $\\theta_{0}$, and add the regularization to all of the others:\n",
        "\n",
        "\n",
        "\n",
        "$\n",
        " \\theta_{j} =  \\theta_{j} - \\alpha \\frac{1}{m} \\sum_{i=1}^m ( h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}_{j} , j=0\n",
        "$\n",
        "\n",
        "\n",
        "\n",
        "$\n",
        " \\theta_{j} =  \\theta_{j} (1 - \\alpha \\frac{\\lambda}{m}) -\n",
        " \\alpha \\frac{1}{m} \\sum_{i=1}^m ( h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}_{j} , j>0\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-h9vdC5hSPO"
      },
      "outputs": [],
      "source": [
        "def mean_squared_error(y_true: torch.Tensor, y_pred: torch.Tensor, lam: float, theta: torch.tensor) -> torch.Tensor:\n",
        "  ### your code here\n",
        "  return\n",
        "\n",
        "def gradient_descent_step(model: nn.Module, X: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor, lr: float, lam: float) -> None:\n",
        "  weight = model.weight\n",
        "  N = X.shape[0]\n",
        "  ### your code here\n",
        "  ###\n",
        "  model.weight = nn.Parameter(weight, requires_grad=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZfVxPRYkns1"
      },
      "source": [
        "<font color=\"maroon\"> **Q8.** First of all, find the best value of alpha to use in order to optimize best.\n",
        "Next, experiment with different values of $\\lambda$ and see how this affects the shape of the hypothesis. [3 marks]</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hFP4DHzZDkB"
      },
      "outputs": [],
      "source": [
        "cost_lst = list()\n",
        "model = LinearRegression(x3.shape[1])\n",
        "alpha = 1 # select an appropriate alpha\n",
        "lam = 0 # select an appropriate lambda\n",
        "for it in range(100):\n",
        "  prediction = model(x3)\n",
        "  cost = mean_squared_error(y, prediction, lam, model.weight)\n",
        "  cost_lst.append(cost)\n",
        "  gradient_descent_step(model, x3, y, prediction, alpha, lam)\n",
        "display.clear_output(wait=True)\n",
        "plt.plot(list(range(it+1)), cost_lst)\n",
        "plt.show()\n",
        "print(model.weight)\n",
        "print('Minimum cost: {}'.format(min(cost_lst)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xys3bBGpk8wp"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x3[:, 0], y, c='red', marker='x', label='groundtruth')\n",
        "outputs = model(x3)\n",
        "plt.plot(x3[:, 0], outputs, c='blue', marker='o', label='prediction')\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('y=f(x1)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
